{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arctic-drink",
   "metadata": {},
   "source": [
    "# Chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-temperature",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-computer",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out Jenny Orr’s nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import choice, random, sample\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reber import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-desert",
   "metadata": {},
   "source": [
    "## Reber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_edges = ((0,1,'B'), (1,2,'T'), (1,3,'P'), (2,2,'S'), (2,4,'X'), (3,3,'T'), (3,5,'V'), (4,3,'X'), (4,6,'S'), (5,4,'P'), (5,6,'V'), (6,None,'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = dict_from_edges(reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = generate_sentence(node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_from_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters(reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_edge = sentence[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence_edge = corrupt_edge(sentence_edge, reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence = corrupt_sentence(sentence, reber_edges, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-postage",
   "metadata": {},
   "source": [
    "## Embedder Reber Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges = ((0,1,'B'), (1,2,'T'), (1,3,'P'), (2,4,reber_edges), (3,5,reber_edges), (4,6, 'T'), (5,6,'P'), (6,None,'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges = flatten_embedded_edges(embedded_reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = dict_from_edges(embedded_reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = generate_sentence(node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_from_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_sentence(sentence, embedded_reber_edges, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-folder",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-pasta",
   "metadata": {},
   "source": [
    "We will write a generator function that produces a reber sentence. With equal probability, the sentence will be corrupted (label 0). If corrupted, the number of corruptions is randonmly determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reber_training_sample(max_corruptions, edges, node_dict, allowed_chars):\n",
    "    sentence = generate_sentence(node_dict)\n",
    "    if random() < .5:\n",
    "        num_corruptions = choice(range(1,max_corruptions+1))\n",
    "        sentence = corrupt_sentence(sentence, edges, num_corruptions)\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    s = string_from_sentence(sentence)\n",
    "    x = string_to_ids(s, allowed_chars)\n",
    "    x = tf.ragged.constant(x, dtype=tf.int8, ragged_rank=0)\n",
    "    y = tf.constant(label, dtype=tf.int8)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_generator(max_corruptions, edges, n=10000):\n",
    "    node_dict = dict_from_edges(edges)\n",
    "    allowed_chars = unique_letters(edges)\n",
    "    for i in range(n):\n",
    "        yield generate_reber_training_sample(max_corruptions, edges, node_dict, allowed_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-manor",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corruptions = 3\n",
    "embedding_size = 5\n",
    "input_dim = len(unique_letters(embedded_reber_edges)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_generator(lambda: training_data_generator(max_corruptions, embedded_reber_edges),\n",
    "                                     output_types=(tf.int8, tf.int8), output_shapes=(tf.TensorShape([None]), tf.TensorShape([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.padded_batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-animal",
   "metadata": {},
   "source": [
    "Let's see how well an LSTM layer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.LSTM(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-oracle",
   "metadata": {},
   "source": [
    "Finally, let's try a SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.SimpleRNN(30, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(30, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rnn = model_rnn.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-final",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-morgan",
   "metadata": {},
   "source": [
    "First, we need a method to generate dates in different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-globe",
   "metadata": {},
   "source": [
    "## Character level seq-to-seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from date_translation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_training_dates(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = list(set(''.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS.index(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dates(x)\n",
    "preprocess_dates(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-burning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = generate_training_data(20000)\n",
    "X_valid, Y_valid = generate_training_data(2000)\n",
    "X_test,  Y_test  = generate_training_data(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "max_char_in = tf.math.reduce_max(X_train).numpy()\n",
    "max_char_out = tf.math.reduce_max(Y_train).numpy()\n",
    "input_length = X_train.shape[1]\n",
    "output_length = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_char_in+1, output_dim=embedding_size, input_shape=[input_length]),\n",
    "    keras.layers.LSTM(128)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(max_char_out+1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(output_length),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-laundry",
   "metadata": {},
   "source": [
    "# Exercise 11\n",
    "_Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sealed-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, seed\n",
    "from tensorflow import keras\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-gibson",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-peninsula",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "GPT-2 uses byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graduate-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invisible-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [10248, 3329, 477, 0], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('Good morning all!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-maximum",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "formal-tobago",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-unknown",
   "metadata": {},
   "source": [
    "## Text generation - No Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "descending-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_lines = shakespeare_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "roman-mechanics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And time it is, when raging war is done,\n"
     ]
    }
   ],
   "source": [
    "seed(142)\n",
    "prompt = choice(shakespeare_lines)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legendary-duncan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1870   640   340   318    11   618 30082  1175   318  1760    11]], shape=(1, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='tf')\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chemical-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 5\n",
    "max_num_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "annual-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "generated_sequences = model.generate(\n",
    "input_ids=encoded_prompt,\n",
    "max_length = max_num_tokens,\n",
    "do_sample = True,\n",
    "temperature=1.0,\n",
    "top_k=0,\n",
    "top_p=0.9,\n",
    "repetition_penalty=1.0,\n",
    "num_return_sequences=num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informal-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And time it is, when raging war is done, it is all but safe.\n",
      "\n",
      "Zuhdi, had you helped the rebels today, should you give them the information you've told them?\n",
      "\n",
      "Yeah, he's caught up.\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done, your precious timbers will no longer hold life.\" He did not pause, but continued saying, \"And this is like to Caesar's hurt shoulders: no matter what the order will be, I\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done, as thou hast done with this country; and when stardom are brought to the point of ruin, as thy midst has been broken in wars, thou hast brought thither for re-m\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done, are those responsible for maintaining peace?\"(13)\n",
      "\n",
      "He continued by saying \"peace-loving nations must persist in support of the higher social cost which cannot be paid by some to the collective\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done,\n",
      "\n",
      "Who, therefore, hath got the hearts, as that place\n",
      "\n",
      "Behold the fire in the fire? Who, however, beheld him then?\n",
      "\n",
      "13 The fire,\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    sentence = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(sentence)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-guitar",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-crowd",
   "metadata": {},
   "source": [
    "Training script for fine tuning language models in huggingface is published [here](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supreme-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(shakespeare_lines) * 90 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "congressional-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_shakespeare.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(shakespeare_lines[:train_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bizarre-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_shakespeare.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(shakespeare_lines[train_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2 \\\n",
    "--model_name_or_path gpt2 \\\n",
    "--train_file \"train_shakespeare.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"valid_shakespeare.txt\" \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 5 \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--output_dir /gpt2_shakespeare/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thrown-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'lm_head.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_finetunes = TFGPT2LMHeadModel.from_pretrained(\"gpt2_shakespeare/\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sustained-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "generated_sequences = model_finetunes.generate(\n",
    "input_ids=encoded_prompt,\n",
    "max_length = max_num_tokens,\n",
    "do_sample = True,\n",
    "temperature=1.0,\n",
    "top_k=0,\n",
    "top_p=0.9,\n",
    "repetition_penalty=1.0,\n",
    "num_return_sequences=num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "naval-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And time it is, when raging war is done,--I am not too much persuaded of you:--I thought it well when we did fight.KING RICHARD II:In all my years of service, I have seen your duty straight\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done,Here we take our daily leave.Go forth and lament our Lord of Somerset,Whilst you hear the clamour,How often he cries and what doth he say;And how his jealous\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done, it fears much: And new things are doth grow stale.KING RICHARD III:My lords and masters,In this day many lights have been banished,But as lately of ages scarce\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done,For you are come with the weapon of thy breastAnd the plume of thy bosom.QUEEN MARGARET:So so is my body, and with the end of thy\n",
      "--------------------------------------------------------------------------------\n",
      "And time it is, when raging war is done, remember I am your traitor!Peace, cousins, peace!Father:Peace!Methinks it is true, our adversaries are our equals.3 KING HENRY VIKING RICHARD\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    sentence = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(sentence)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-economics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
