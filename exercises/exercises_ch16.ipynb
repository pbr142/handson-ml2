{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-logging",
   "metadata": {},
   "source": [
    "# Chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-repair",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-phase",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out Jenny Orr’s nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import choice, random, sample\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reber import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-guess",
   "metadata": {},
   "source": [
    "## Reber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_edges = ((0,1,'B'), (1,2,'T'), (1,3,'P'), (2,2,'S'), (2,4,'X'), (3,3,'T'), (3,5,'V'), (4,3,'X'), (4,6,'S'), (5,4,'P'), (5,6,'V'), (6,None,'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = dict_from_edges(reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = generate_sentence(node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_from_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters(reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_edge = sentence[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence_edge = corrupt_edge(sentence_edge, reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence = corrupt_sentence(sentence, reber_edges, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-detector",
   "metadata": {},
   "source": [
    "## Embedder Reber Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges = ((0,1,'B'), (1,2,'T'), (1,3,'P'), (2,4,reber_edges), (3,5,reber_edges), (4,6, 'T'), (5,6,'P'), (6,None,'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges = flatten_embedded_edges(embedded_reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = dict_from_edges(embedded_reber_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = generate_sentence(node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_from_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_sentence(sentence, embedded_reber_edges, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-tuesday",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-activation",
   "metadata": {},
   "source": [
    "We will write a generator function that produces a reber sentence. With equal probability, the sentence will be corrupted (label 0). If corrupted, the number of corruptions is randonmly determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reber_training_sample(max_corruptions, edges, node_dict, allowed_chars):\n",
    "    sentence = generate_sentence(node_dict)\n",
    "    if random() < .5:\n",
    "        num_corruptions = choice(range(1,max_corruptions+1))\n",
    "        sentence = corrupt_sentence(sentence, edges, num_corruptions)\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    s = string_from_sentence(sentence)\n",
    "    x = string_to_ids(s, allowed_chars)\n",
    "    x = tf.ragged.constant(x, dtype=tf.int8, ragged_rank=0)\n",
    "    y = tf.constant(label, dtype=tf.int8)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_generator(max_corruptions, edges, n=10000):\n",
    "    node_dict = dict_from_edges(edges)\n",
    "    allowed_chars = unique_letters(edges)\n",
    "    for i in range(n):\n",
    "        yield generate_reber_training_sample(max_corruptions, edges, node_dict, allowed_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-california",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corruptions = 3\n",
    "embedding_size = 5\n",
    "input_dim = len(unique_letters(embedded_reber_edges)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_generator(lambda: training_data_generator(max_corruptions, embedded_reber_edges),\n",
    "                                     output_types=(tf.int8, tf.int8), output_shapes=(tf.TensorShape([None]), tf.TensorShape([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.padded_batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-demographic",
   "metadata": {},
   "source": [
    "Let's see how well an LSTM layer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.LSTM(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-biotechnology",
   "metadata": {},
   "source": [
    "Finally, let's try a SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.SimpleRNN(30, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(30, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rnn = model_rnn.fit(data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-discharge",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-tampa",
   "metadata": {},
   "source": [
    "First, we need a method to generate dates in different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-liverpool",
   "metadata": {},
   "source": [
    "## Character level seq-to-seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from date_translation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_training_dates(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = list(set(''.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS.index(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dates(x)\n",
    "preprocess_dates(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-symphony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-tucson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-uruguay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = generate_training_data(20000)\n",
    "X_valid, Y_valid = generate_training_data(2000)\n",
    "X_test,  Y_test  = generate_training_data(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "max_char_in = tf.math.reduce_max(X_train).numpy()\n",
    "max_char_out = tf.math.reduce_max(Y_train).numpy()\n",
    "input_length = X_train.shape[1]\n",
    "output_length = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_char_in+1, output_dim=embedding_size, input_shape=[input_length]),\n",
    "    keras.layers.LSTM(128)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(max_char_out+1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(output_length),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-australia",
   "metadata": {},
   "source": [
    "# Exercise 11\n",
    "_Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, seed\n",
    "from tensorflow import keras\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, TFTrainer, TextDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-jonathan",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-arcade",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "GPT-2 uses byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer('Good morning all!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-minnesota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rational-composer",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-cleveland",
   "metadata": {},
   "source": [
    "## Text generation - No Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_lines = shakespeare_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(142)\n",
    "prompt = choice(shakespeare_lines)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='tf')\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 5\n",
    "max_num_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences = model.generate(\n",
    "input_ids=encoded_prompt,\n",
    "max_length = max_num_tokens + len(encoded_prompt),\n",
    "do_sample = True,\n",
    "temperature=1.0,\n",
    "top_k=0,\n",
    "top_p=0.9,\n",
    "repetition_penalty=1.0,\n",
    "num_return_sequences=num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in generated_sequences:\n",
    "    sentence = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(sentence)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-transsexual",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-death",
   "metadata": {},
   "source": [
    "Training script for fine tuning language models in huggingface is published [here](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informational-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fiscal-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wooden-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_sequences = shakespeare_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "funny-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(shakespeare_sequences) * 90 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designed-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_shakespeare.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(shakespeare_sequences[:train_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mexican-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_shakespeare.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(shakespeare_sequences[train_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_shakespeare.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"valid_shakespeare.txt\" \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 5 \\\n",
    "--output_dir /gpt2_shakepeare/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-shannon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-theta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c huggingface -c conda-forge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "        tokenizer,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-color",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-latter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=filepath,\n",
    "    block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-mistake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
